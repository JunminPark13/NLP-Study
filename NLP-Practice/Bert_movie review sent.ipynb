{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Bert_movie review sent.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"193cc6946ac249fd9518cac516359f6b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_2b316798c7b8457db8b5cda1266e69c8","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_d5656be5878d477fbb9a665a9b2c63ac","IPY_MODEL_5f6eed3f63af4ef5990c00e5ecb569e2"]}},"2b316798c7b8457db8b5cda1266e69c8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d5656be5878d477fbb9a665a9b2c63ac":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_0ff528d3e9724cd7ad0e2d6b4fb87735","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":995526,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":995526,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d21c72b0f2bd43a49ee05a1f78aa5940"}},"5f6eed3f63af4ef5990c00e5ecb569e2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_53049f4b0b3a45b19c9be2bdd5cd20da","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 996k/996k [00:00&lt;00:00, 2.07MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e7f7911989154650b2b3d40a0d772562"}},"0ff528d3e9724cd7ad0e2d6b4fb87735":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"d21c72b0f2bd43a49ee05a1f78aa5940":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"53049f4b0b3a45b19c9be2bdd5cd20da":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"e7f7911989154650b2b3d40a0d772562":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8f1acda940a04c5f943b29a2cca469d4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_602420430570406e9dac54af112608d5","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_15ce1509d4c14fcc95208dbf3afe5f10","IPY_MODEL_0b2f3ffbf8ad4222a35c9bc421f0372e"]}},"602420430570406e9dac54af112608d5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"15ce1509d4c14fcc95208dbf3afe5f10":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_5581b075f7034547b1a5ec501c7e5d5b","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":625,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":625,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_73501e8539c14bb08f40c7e35e778a7e"}},"0b2f3ffbf8ad4222a35c9bc421f0372e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_5ce8a00f8fdd4085bfc8b1ab867fa879","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 625/625 [00:19&lt;00:00, 32.7B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_cd03b028b4b14194aa7251dc0b9db01e"}},"5581b075f7034547b1a5ec501c7e5d5b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"73501e8539c14bb08f40c7e35e778a7e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5ce8a00f8fdd4085bfc8b1ab867fa879":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"cd03b028b4b14194aa7251dc0b9db01e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d93f52dad56e4d4a93a48551faaffc82":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_579ed825199c490c9869dee286ae6196","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_ba822a73477a48aaa2e97ea245307dd5","IPY_MODEL_221d7c71abf24998942602fbe054e43e"]}},"579ed825199c490c9869dee286ae6196":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ba822a73477a48aaa2e97ea245307dd5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_2b814f1c62234681ad12d7b7bbfb8f35","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":714314041,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":714314041,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_645b5a8d50e3432ea3985dfdc6037d9b"}},"221d7c71abf24998942602fbe054e43e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_5a530a50ef354b3f8299d23e738f395c","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 714M/714M [00:14&lt;00:00, 49.3MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_def94990c23c4e74ad5464b0a1af6337"}},"2b814f1c62234681ad12d7b7bbfb8f35":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"645b5a8d50e3432ea3985dfdc6037d9b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5a530a50ef354b3f8299d23e738f395c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"def94990c23c4e74ad5464b0a1af6337":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iA9vb79oHICl","executionInfo":{"status":"ok","timestamp":1614605189302,"user_tz":-540,"elapsed":8386,"user":{"displayName":"‍박준민(학부학생/상경대학 응용통계학과)","photoUrl":"","userId":"13006511384417057537"}},"outputId":"64154c0a-70ed-4fdf-e059-5a56dbdc76da"},"source":["!pip install transformers"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f9/54/5ca07ec9569d2f232f3166de5457b63943882f7950ddfcc887732fc7fb23/transformers-4.3.3-py3-none-any.whl (1.9MB)\n","\u001b[K     |████████████████████████████████| 1.9MB 7.8MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 37.5MB/s \n","\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/23/2ddc317b2121117bf34dd00f5b0de194158f2a44ee2bf5e47c7166878a97/tokenizers-0.10.1-cp37-cp37m-manylinux2010_x86_64.whl (3.2MB)\n","\u001b[K     |████████████████████████████████| 3.2MB 48.7MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp37-none-any.whl size=893262 sha256=34e77b39be748c3105f2fc9d858ccca93ee1ad2a1b59df34a6645d1efb3c2611\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: sacremoses, tokenizers, transformers\n","Successfully installed sacremoses-0.0.43 tokenizers-0.10.1 transformers-4.3.3\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FJCPupR_HYxD"},"source":["import tensorflow as tf\r\n","import torch\r\n","\r\n","from transformers import BertTokenizer\r\n","from transformers import BertForSequenceClassification, AdamW, BertConfig\r\n","from transformers import get_linear_schedule_with_warmup\r\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\r\n","from keras.preprocessing.sequence import pad_sequences\r\n","from sklearn.model_selection import train_test_split\r\n","\r\n","import pandas as pd\r\n","import numpy as np\r\n","import random\r\n","import time\r\n","import datetime\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8TvfjWg7Uzkx"},"source":["### 데이터 가져오기"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bi91SiZdI6sc","executionInfo":{"status":"ok","timestamp":1614605215849,"user_tz":-540,"elapsed":8967,"user":{"displayName":"‍박준민(학부학생/상경대학 응용통계학과)","photoUrl":"","userId":"13006511384417057537"}},"outputId":"99d088b6-fd06-422f-e4bc-67772f3069f8"},"source":["!git clone https://github.com/e9t/nsmc.git"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Cloning into 'nsmc'...\n","remote: Enumerating objects: 14763, done.\u001b[K\n","remote: Total 14763 (delta 0), reused 0 (delta 0), pack-reused 14763\u001b[K\n","Receiving objects: 100% (14763/14763), 56.19 MiB | 19.54 MiB/s, done.\n","Resolving deltas: 100% (1749/1749), done.\n","Checking out files: 100% (14737/14737), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6CTBGRlaJJr9","executionInfo":{"status":"ok","timestamp":1614587290262,"user_tz":-540,"elapsed":9891,"user":{"displayName":"박준민","photoUrl":"","userId":"03182793311460293895"}},"outputId":"7e1c14c9-47b7-4533-fc0f-ee2be26f5e27"},"source":["!ls nsmc -la"],"execution_count":null,"outputs":[{"output_type":"stream","text":["total 38644\n","drwxr-xr-x 5 root root     4096 Mar  1 08:28 .\n","drwxr-xr-x 1 root root     4096 Mar  1 08:28 ..\n","drwxr-xr-x 2 root root     4096 Mar  1 08:28 code\n","drwxr-xr-x 8 root root     4096 Mar  1 08:28 .git\n","-rw-r--r-- 1 root root  4893335 Mar  1 08:28 ratings_test.txt\n","-rw-r--r-- 1 root root 14628807 Mar  1 08:28 ratings_train.txt\n","-rw-r--r-- 1 root root 19515078 Mar  1 08:28 ratings.txt\n","drwxr-xr-x 2 root root   466944 Mar  1 08:28 raw\n","-rw-r--r-- 1 root root     2596 Mar  1 08:28 README.md\n","-rw-r--r-- 1 root root    36746 Mar  1 08:28 synopses.json\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"izC7blxvJSep","executionInfo":{"status":"ok","timestamp":1614605223241,"user_tz":-540,"elapsed":765,"user":{"displayName":"‍박준민(학부학생/상경대학 응용통계학과)","photoUrl":"","userId":"13006511384417057537"}},"outputId":"74d4531d-7814-4ebc-f9f3-47f59f3c79fc"},"source":["train = pd.read_csv('nsmc/ratings_train.txt', sep='\\t')\r\n","test = pd.read_csv('nsmc/ratings_test.txt', sep='\\t')\r\n","\r\n","print(train.shape)\r\n","print(test.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(150000, 3)\n","(50000, 3)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M8MTkUzC-JV1","executionInfo":{"status":"ok","timestamp":1614605291709,"user_tz":-540,"elapsed":977,"user":{"displayName":"‍박준민(학부학생/상경대학 응용통계학과)","photoUrl":"","userId":"13006511384417057537"}},"outputId":"b1a656cb-9fab-4052-de6c-65e42378f4f0"},"source":["train = train[:70000]\r\n","test = test[:20000]\r\n","\r\n","print(train.shape)\r\n","print(test.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(70000, 3)\n","(20000, 3)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"oNcWnPSdUtv0"},"source":["### train data 전처리"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":194},"id":"Znj7VWtsJp41","executionInfo":{"status":"ok","timestamp":1614605298811,"user_tz":-540,"elapsed":1085,"user":{"displayName":"‍박준민(학부학생/상경대학 응용통계학과)","photoUrl":"","userId":"13006511384417057537"}},"outputId":"62a8e78c-a4a7-4d47-baa3-c26362eed073"},"source":["train.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>document</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>9976970</td>\n","      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>3819312</td>\n","      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>10265843</td>\n","      <td>너무재밓었다그래서보는것을추천한다</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>9045019</td>\n","      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>6483659</td>\n","      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         id                                           document  label\n","0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n","1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n","2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n","3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n","4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NHs3fQOYKIJH","executionInfo":{"status":"ok","timestamp":1614605302178,"user_tz":-540,"elapsed":622,"user":{"displayName":"‍박준민(학부학생/상경대학 응용통계학과)","photoUrl":"","userId":"13006511384417057537"}},"outputId":"760df1c9-07fa-4995-acc1-8006ed413776"},"source":["# 리뷰 문장 추출\r\n","\r\n","sentences = train['document']\r\n","sentences[:10]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0                                  아 더빙.. 진짜 짜증나네요 목소리\n","1                    흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나\n","2                                    너무재밓었다그래서보는것을추천한다\n","3                        교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정\n","4    사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...\n","5        막 걸음마 뗀 3세부터 초등학교 1학년생인 8살용영화.ㅋㅋㅋ...별반개도 아까움.\n","6                                원작의 긴장감을 제대로 살려내지못했다.\n","7    별 반개도 아깝다 욕나온다 이응경 길용우 연기생활이몇년인지..정말 발로해도 그것보단...\n","8                               액션이 없는데도 재미 있는 몇안되는 영화\n","9        왜케 평점이 낮은건데? 꽤 볼만한데.. 헐리우드식 화려함에만 너무 길들여져 있나?\n","Name: document, dtype: object"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B2vI18abLHRa","executionInfo":{"status":"ok","timestamp":1614605317778,"user_tz":-540,"elapsed":13247,"user":{"displayName":"‍박준민(학부학생/상경대학 응용통계학과)","photoUrl":"","userId":"13006511384417057537"}},"outputId":"3fd45d87-961e-41db-d6e6-7bb227536804"},"source":["# BERT 입력 형식에 맞게 변환\r\n","\r\n","sentences = ['[CLS] ' + str(sentence) +' [SEP]' for sentence in sentences]\r\n","sentences[:10]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['[CLS] 아 더빙.. 진짜 짜증나네요 목소리 [SEP]',\n"," '[CLS] 흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나 [SEP]',\n"," '[CLS] 너무재밓었다그래서보는것을추천한다 [SEP]',\n"," '[CLS] 교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정 [SEP]',\n"," '[CLS] 사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 던스트가 너무나도 이뻐보였다 [SEP]',\n"," '[CLS] 막 걸음마 뗀 3세부터 초등학교 1학년생인 8살용영화.ㅋㅋㅋ...별반개도 아까움. [SEP]',\n"," '[CLS] 원작의 긴장감을 제대로 살려내지못했다. [SEP]',\n"," '[CLS] 별 반개도 아깝다 욕나온다 이응경 길용우 연기생활이몇년인지..정말 발로해도 그것보단 낫겟다 납치.감금만반복반복..이드라마는 가족도없다 연기못하는사람만모엿네 [SEP]',\n"," '[CLS] 액션이 없는데도 재미 있는 몇안되는 영화 [SEP]',\n"," '[CLS] 왜케 평점이 낮은건데? 꽤 볼만한데.. 헐리우드식 화려함에만 너무 길들여져 있나? [SEP]']"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9sIkGQBOLi93","executionInfo":{"status":"ok","timestamp":1614605325200,"user_tz":-540,"elapsed":675,"user":{"displayName":"‍박준민(학부학생/상경대학 응용통계학과)","photoUrl":"","userId":"13006511384417057537"}},"outputId":"19274cfc-241d-42fe-dc32-86a910b43ed0"},"source":["labels = train['label'].values\r\n","labels"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0, 1, 0, ..., 0, 1, 0])"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":102,"referenced_widgets":["193cc6946ac249fd9518cac516359f6b","2b316798c7b8457db8b5cda1266e69c8","d5656be5878d477fbb9a665a9b2c63ac","5f6eed3f63af4ef5990c00e5ecb569e2","0ff528d3e9724cd7ad0e2d6b4fb87735","d21c72b0f2bd43a49ee05a1f78aa5940","53049f4b0b3a45b19c9be2bdd5cd20da","e7f7911989154650b2b3d40a0d772562"]},"id":"BAIxAzqnL4Mh","executionInfo":{"status":"ok","timestamp":1614605340864,"user_tz":-540,"elapsed":14069,"user":{"displayName":"‍박준민(학부학생/상경대학 응용통계학과)","photoUrl":"","userId":"13006511384417057537"}},"outputId":"55b31a90-608f-44f4-fd95-ea3ba624e1f4"},"source":["# Tokenizing\r\n","\r\n","tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)\r\n","tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\r\n","\r\n","print(sentences[0])\r\n","print(tokenized_texts[0])"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"193cc6946ac249fd9518cac516359f6b","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=995526.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","[CLS] 아 더빙.. 진짜 짜증나네요 목소리 [SEP]\n","['[CLS]', '아', '더', '##빙', '.', '.', '진', '##짜', '짜', '##증', '##나', '##네', '##요', '목', '##소', '##리', '[SEP]']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uyeiVCRNMfUV","executionInfo":{"status":"ok","timestamp":1614605430947,"user_tz":-540,"elapsed":2786,"user":{"displayName":"‍박준민(학부학생/상경대학 응용통계학과)","photoUrl":"","userId":"13006511384417057537"}},"outputId":"cbb3b8e4-6f03-4120-8b97-9e05fee930fe"},"source":["MAX_LEN = 128 # 입력 토큰의 최대 seq 길이\r\n","input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts] # 토큰을 숫자 인덱스로\r\n","input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype='long', truncating='post', padding='post') # max_len에 맞게 문장 자르고 모자란 부분 패딩 0으로 채움\r\n","\r\n","input_ids[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([   101,   9519,   9074, 119005,    119,    119,   9708, 119235,\n","         9715, 119230,  16439,  77884,  48549,   9284,  22333,  12692,\n","          102,      0,      0,      0,      0,      0,      0,      0,\n","            0,      0,      0,      0,      0,      0,      0,      0,\n","            0,      0,      0,      0,      0,      0,      0,      0,\n","            0,      0,      0,      0,      0,      0,      0,      0,\n","            0,      0,      0,      0,      0,      0,      0,      0,\n","            0,      0,      0,      0,      0,      0,      0,      0,\n","            0,      0,      0,      0,      0,      0,      0,      0,\n","            0,      0,      0,      0,      0,      0,      0,      0,\n","            0,      0,      0,      0,      0,      0,      0,      0,\n","            0,      0,      0,      0,      0,      0,      0,      0,\n","            0,      0,      0,      0,      0,      0,      0,      0,\n","            0,      0,      0,      0,      0,      0,      0,      0,\n","            0,      0,      0,      0,      0,      0,      0,      0,\n","            0,      0,      0,      0,      0,      0,      0,      0])"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MJy-PjlLNrGc","executionInfo":{"status":"ok","timestamp":1614605439674,"user_tz":-540,"elapsed":6675,"user":{"displayName":"‍박준민(학부학생/상경대학 응용통계학과)","photoUrl":"","userId":"13006511384417057537"}},"outputId":"0036a6dc-d18f-47cb-af01-b7a625c65c8f"},"source":["attention_masks = [] # attention_mask 초기화\r\n","\r\n","# 어텐션 마스크를 패딩이 아니면 1, 패딩이면 0으로 설정\r\n","# 패딩 부분은 BERT 모델에서 어텐션을 수행하지 않아 속도 향상\r\n","for seq in input_ids:\r\n","    seq_mask = [float(i>0) for i in seq]\r\n","    attention_masks.append(seq_mask)\r\n","\r\n","print(attention_masks[0])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A_nWizEQOm9B","executionInfo":{"status":"ok","timestamp":1614605442887,"user_tz":-540,"elapsed":1426,"user":{"displayName":"‍박준민(학부학생/상경대학 응용통계학과)","photoUrl":"","userId":"13006511384417057537"}},"outputId":"ce15fabc-caad-4b2c-b39c-d5460ac97bac"},"source":["# train_test_split\r\n","\r\n","train_inputs, val_inputs, train_lab, val_lab = train_test_split(input_ids, labels, random_state=2018, test_size=0.1)\r\n","\r\n","train_masks, val_masks, _, _ = train_test_split(attention_masks, input_ids, random_state=2018, test_size=0.1)\r\n","\r\n","train_inputs = torch.tensor(train_inputs)\r\n","train_lab = torch.tensor(train_lab)\r\n","train_masks = torch.tensor(train_masks)\r\n","val_inputs = torch.tensor(val_inputs)\r\n","val_lab = torch.tensor(val_lab)\r\n","val_masks = torch.tensor(val_masks)\t\t\t\t\r\n","\r\n","print(train_inputs[0])\r\n","print(train_lab[0])\r\n","print(train_masks[0])\r\n","print(val_inputs[0])\r\n","print(val_lab[0])\r\n","print(val_masks[0])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([  101, 91395, 61844, 11261,  8962, 54867, 18108,  9358, 85533,  9638,\n","        71439,  9511, 56645, 12310,   102,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0])\n","tensor(0)\n","tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0.])\n","tensor([   101,   9708, 119235,   9659,  22458, 119136,  11903,   9303,   9638,\n","        118827, 118624,  42428,  59894,   9708, 119235,    100,    102,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0])\n","tensor(0)\n","tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0.])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vYhUcyylQmUq"},"source":["batch_size = 32\r\n","\r\n","# 파이토치의 DataLoader로 입력, 마스크, 라벨을 묶어 데이터 설정\r\n","# 학습시 배치 사이즈 만큼 데이터를 가져옴\r\n","train_data = TensorDataset(train_inputs, train_masks, train_lab)\r\n","train_sampler = RandomSampler(train_data)\r\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\r\n","\r\n","val_data = TensorDataset(val_inputs, val_masks,val_lab)\r\n","val_sampler = SequentialSampler(val_data)\r\n","val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":194},"id":"SGKLIpG9RzHt","executionInfo":{"status":"ok","timestamp":1614605451423,"user_tz":-540,"elapsed":577,"user":{"displayName":"‍박준민(학부학생/상경대학 응용통계학과)","photoUrl":"","userId":"13006511384417057537"}},"outputId":"75c1dc01-dd47-48ab-ab16-a66798447660"},"source":["test.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>document</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>6270596</td>\n","      <td>굳 ㅋ</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>9274899</td>\n","      <td>GDNTOPCLASSINTHECLUB</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>8544678</td>\n","      <td>뭐야 이 평점들은.... 나쁘진 않지만 10점 짜리는 더더욱 아니잖아</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>6825595</td>\n","      <td>지루하지는 않은데 완전 막장임... 돈주고 보기에는....</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>6723715</td>\n","      <td>3D만 아니었어도 별 다섯 개 줬을텐데.. 왜 3D로 나와서 제 심기를 불편하게 하죠??</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        id                                           document  label\n","0  6270596                                                굳 ㅋ      1\n","1  9274899                               GDNTOPCLASSINTHECLUB      0\n","2  8544678             뭐야 이 평점들은.... 나쁘진 않지만 10점 짜리는 더더욱 아니잖아      0\n","3  6825595                   지루하지는 않은데 완전 막장임... 돈주고 보기에는....      0\n","4  6723715  3D만 아니었어도 별 다섯 개 줬을텐데.. 왜 3D로 나와서 제 심기를 불편하게 하죠??      0"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"jQNJjkAuU255"},"source":["### test dat 전처리"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WERoeXqeSVWz","executionInfo":{"status":"ok","timestamp":1614605461875,"user_tz":-540,"elapsed":6836,"user":{"displayName":"‍박준민(학부학생/상경대학 응용통계학과)","photoUrl":"","userId":"13006511384417057537"}},"outputId":"8e991820-b5b6-45c1-d0e5-6c6f265e851c"},"source":["sentences = test['document']\r\n","\r\n","sentences = ['[CLS] ' + str(sent) + ' [SEP]' for sent in sentences]\r\n","\r\n","labels = test['label'].values\r\n","\r\n","tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)\r\n","tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\r\n","\r\n","MAX_LEN = 128 \r\n","input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\r\n","input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype='long', truncating='post', padding='post')\r\n","\r\n","attention_masks = []\r\n","\r\n","for seq in input_ids:\r\n","    seq_mask = [float(i>0) for i in seq]\r\n","    attention_masks.append(seq_mask)\r\n","\r\n","test_inputs = torch.tensor(input_ids)\r\n","test_labels = torch.tensor(labels)\r\n","test_masks = torch.tensor(attention_masks)\r\n","\r\n","print(test_inputs[0])\r\n","print(test_labels[0])\r\n","print(test_masks[0])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([ 101, 8911,  100,  102,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0])\n","tensor(1)\n","tensor([1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0.])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VzYEWOzQSwUT"},"source":["batch_size = 32\r\n","\r\n","test_data = TensorDataset(test_inputs, test_masks, test_labels)\r\n","test_sampler = RandomSampler(test_data)\r\n","test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"417637GwU5k1"},"source":["### 모델 생성"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fQcYCTaMUprA","executionInfo":{"status":"ok","timestamp":1614605478467,"user_tz":-540,"elapsed":5899,"user":{"displayName":"‍박준민(학부학생/상경대학 응용통계학과)","photoUrl":"","userId":"13006511384417057537"}},"outputId":"c55f9dfc-5cb0-4bb3-ab4a-ee2defebc1a8"},"source":["# GPU 디바이스 이름 구함\r\n","device_name = tf.test.gpu_device_name()\r\n","\r\n","# GPU 디바이스 이름 검사\r\n","if device_name == '/device:GPU:0':\r\n","    print('Found GPU at: {}'.format(device_name))\r\n","else:\r\n","    raise SystemError('GPU device not found')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Found GPU at: /device:GPU:0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1Y66FpF6VHmL","executionInfo":{"status":"ok","timestamp":1614605482790,"user_tz":-540,"elapsed":682,"user":{"displayName":"‍박준민(학부학생/상경대학 응용통계학과)","photoUrl":"","userId":"13006511384417057537"}},"outputId":"7cd90547-788e-4a4f-972e-e7f68b80657d"},"source":["# 디바이스 설정\r\n","\r\n","if torch.cuda.is_available():    \r\n","    device = torch.device(\"cuda\")\r\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\r\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\r\n","else:\r\n","    device = torch.device(\"cpu\")\r\n","    print('No GPU available, using the CPU instead.')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["There are 1 GPU(s) available.\n","We will use the GPU: Tesla T4\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["8f1acda940a04c5f943b29a2cca469d4","602420430570406e9dac54af112608d5","15ce1509d4c14fcc95208dbf3afe5f10","0b2f3ffbf8ad4222a35c9bc421f0372e","5581b075f7034547b1a5ec501c7e5d5b","73501e8539c14bb08f40c7e35e778a7e","5ce8a00f8fdd4085bfc8b1ab867fa879","cd03b028b4b14194aa7251dc0b9db01e","d93f52dad56e4d4a93a48551faaffc82","579ed825199c490c9869dee286ae6196","ba822a73477a48aaa2e97ea245307dd5","221d7c71abf24998942602fbe054e43e","2b814f1c62234681ad12d7b7bbfb8f35","645b5a8d50e3432ea3985dfdc6037d9b","5a530a50ef354b3f8299d23e738f395c","def94990c23c4e74ad5464b0a1af6337"]},"id":"u2dZxyF8VSwA","executionInfo":{"status":"ok","timestamp":1614605527664,"user_tz":-540,"elapsed":31118,"user":{"displayName":"‍박준민(학부학생/상경대학 응용통계학과)","photoUrl":"","userId":"13006511384417057537"}},"outputId":"3820f9bb-3afe-4ca4-9a0b-88dc6daebd6f"},"source":["# BERT 모델 생성\r\n","\r\n","model = BertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=2)\r\n","model.cuda()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8f1acda940a04c5f943b29a2cca469d4","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=625.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d93f52dad56e4d4a93a48551faaffc82","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=714314041.0, style=ProgressStyle(descri…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["BertForSequenceClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"Am-zIW8qVdJN"},"source":["optimizer = AdamW(model.parameters(),\r\n","                  lr = 2e-5,\r\n","                  eps=1e-8)\r\n","\r\n","epochs = 2\r\n","\r\n","total_steps = len(train_dataloader) * epochs\r\n","\r\n","scheduler = get_linear_schedule_with_warmup(optimizer,\r\n","                                            num_warmup_steps=0,\r\n","                                            num_training_steps=total_steps)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DyuofzTDZDud"},"source":["### 모델 학습"]},{"cell_type":"code","metadata":{"id":"X3MgZ4YHYplq"},"source":["def flat_accuracy(preds, labels):\r\n","    pred_flat = np.argmax(preds, axis=1).flatten()\r\n","    labels_flat = labels.flatten()\r\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)\r\n","\r\n","def format_time(elapsed):\r\n","    elapsed_rounded = int(round((elapsed)))\r\n","    return str(datetime.timedelta(seconds=elapsed_rounded))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JIrGEwfcaHMR","executionInfo":{"status":"ok","timestamp":1614609956126,"user_tz":-540,"elapsed":2972080,"user":{"displayName":"‍박준민(학부학생/상경대학 응용통계학과)","photoUrl":"","userId":"13006511384417057537"}},"outputId":"77d86854-11a9-4483-aca3-8bead652b366"},"source":["# 랜덤시드 고정\r\n","seed = 42\r\n","random.seed(seed)\r\n","np.random.seed(seed)\r\n","torch.manual_seed(seed)\r\n","torch.cuda.manual_seed_all(seed)\r\n","\r\n","model.zero_grad()\r\n","\r\n","for epoch_i in range(0, epochs):\r\n","\r\n","    print('')\r\n","    print('====== Epoch {:} / {:} ======'.format(epoch_i +1, epochs))\r\n","    print('Training...')\r\n","\r\n","    t0 = time.time()\r\n","\r\n","    total_loss = 0\r\n","\r\n","    model.train()\r\n","\r\n","    for step, batch in enumerate(train_dataloader):\r\n","        if step % 500 == 0 and not step == 0: # 경과표시\r\n","            elapsed = format_time(time.time() - t0)\r\n","            print(' Batch {:>5,} of {:>5,}.  Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\r\n","        \r\n","        batch = tuple(t.to(device) for t in batch) # 배치를 GPU로\r\n","        b_input_ids, b_input_mask, b_labels = batch # 배치에서 데이터 추출\r\n","\r\n","        # forward 수행\r\n","        outputs = model(b_input_ids, token_type_ids=None,\r\n","                        attention_mask = b_input_mask,\r\n","                        labels = b_labels)\r\n","        \r\n","        loss = outputs[0]\r\n","        total_loss += loss.item()\r\n","        loss.backward()\r\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # 그래디언트 클리핑\r\n","        optimizer.step()\r\n","        scheduler.step()\r\n","        model.zero_grad()\r\n","\r\n","    # 평균 로스 계산\r\n","    avg_train_loss = total_loss / len(train_dataloader)\r\n","\r\n","    print('')\r\n","    print(' Average training loss: {0:.2f}'.format(avg_train_loss))\r\n","    print(' Training epoch took: {:}'.format(format_time(time.time() - t0)))\r\n","\r\n","    print('')\r\n","    print('Running Validation...')\r\n","\r\n","    t0 = time.time()\r\n","    model.eval()\r\n","\r\n","    # 변수 초기화\r\n","    eval_loss, eval_accuracy = 0, 0\r\n","    nb_eval_steps, nb_eval_examples = 0, 0\r\n","\r\n","    for batch in val_dataloader:\r\n","        batch = tuple(t.to(device) for t in batch)\r\n","        b_input_ids, b_input_mask, b_labels = batch\r\n","\r\n","        with torch.no_grad():\r\n","            outputs = model(b_input_ids, \r\n","                            token_type_ids=None, \r\n","                            attention_mask=b_input_mask)\r\n","        \r\n","        logits = outputs[0]\r\n","        logits = logits.detach().cpu().numpy()\r\n","        label_ids = b_labels.to('cpu').numpy()\r\n","\r\n","        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\r\n","        eval_accuracy += tmp_eval_accuracy\r\n","        nb_eval_steps += 1\r\n","    \r\n","    print(' Accuracy: {0:.2f}'.format(eval_accuracy/nb_eval_steps))\r\n","    print(' Validation took: {:}'.format(format_time(time.time() -t0)))\r\n","\r\n","print('')\r\n","print('Training Complete!')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","====== Epoch 1 / 2 ======\n","Training...\n"," Batch   500 of 1,969.  Elapsed: 0:06:04.\n"," Batch 1,000 of 1,969.  Elapsed: 0:12:07.\n"," Batch 1,500 of 1,969.  Elapsed: 0:18:10.\n","\n"," Average training loss: 0.20\n"," Training epoch took: 0:23:50\n","\n","Running Validation...\n"," Accuracy: 0.85\n"," Validation took: 0:00:56\n","\n","====== Epoch 2 / 2 ======\n","Training...\n"," Batch   500 of 1,969.  Elapsed: 0:06:03.\n"," Batch 1,000 of 1,969.  Elapsed: 0:12:06.\n"," Batch 1,500 of 1,969.  Elapsed: 0:18:09.\n","\n"," Average training loss: 0.28\n"," Training epoch took: 0:23:49\n","\n","Running Validation...\n"," Accuracy: 0.85\n"," Validation took: 0:00:56\n","\n","Training Complete!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"nJ6VVmo7E-4L"},"source":["### 테스트셋 평가"]},{"cell_type":"code","metadata":{"id":"IcRC_MMdjGh9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614610179474,"user_tz":-540,"elapsed":157807,"user":{"displayName":"‍박준민(학부학생/상경대학 응용통계학과)","photoUrl":"","userId":"13006511384417057537"}},"outputId":"fb9abb4a-97c7-48d8-c983-bcd761486c86"},"source":["\r\n","t0 = time.time()\r\n","\r\n","model.eval()\r\n","\r\n","eval_loss, eval_accuracy = 0, 0\r\n","nb_eval_steps, nb_eval_examples = 0, 0\r\n","\r\n","for step, batch in enumerate(test_dataloader):\r\n","\r\n","    if step % 100 == 0 and not step == 0:\r\n","        elapsed = format_time(time.time() - t0)\r\n","        print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(test_dataloader), elapsed))\r\n","\r\n","    batch = tuple(t.to(device) for t in batch)\r\n","    b_input_ids, b_input_mask, b_labels = batch\r\n","\r\n","    with torch.no_grad():     \r\n","        outputs = model(b_input_ids, \r\n","                        token_type_ids=None, \r\n","                        attention_mask=b_input_mask)\r\n","    \r\n","    logits = outputs[0]\r\n","\r\n","    logits = logits.detach().cpu().numpy()\r\n","    label_ids = b_labels.to('cpu').numpy()\r\n","\r\n","    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\r\n","    eval_accuracy += tmp_eval_accuracy\r\n","    nb_eval_steps += 1\r\n","\r\n","print(\"\")\r\n","print(\"Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\r\n","print(\"Test took: {:}\".format(format_time(time.time() - t0)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["  Batch   100  of    625.    Elapsed: 0:00:24.\n","  Batch   200  of    625.    Elapsed: 0:00:49.\n","  Batch   300  of    625.    Elapsed: 0:01:14.\n","  Batch   400  of    625.    Elapsed: 0:01:40.\n","  Batch   500  of    625.    Elapsed: 0:02:05.\n","  Batch   600  of    625.    Elapsed: 0:02:31.\n","\n","Accuracy: 0.85\n","Test took: 0:02:37\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"EqskR63mFhBC"},"source":["### 새로운 문장 테스트"]},{"cell_type":"code","metadata":{"id":"duitobGOFLh2"},"source":["# 입력 데이터 변환\r\n","\r\n","def convert_input_data(sentences):\r\n","\r\n","    # BERT의 토크나이저로 문장을 토큰으로 분리\r\n","    tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\r\n","\r\n","    # 입력 토큰의 최대 시퀀스 길이\r\n","    MAX_LEN = 128\r\n","\r\n","    # 토큰을 숫자 인덱스로 변환\r\n","    input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\r\n","    \r\n","    # 문장을 MAX_LEN 길이에 맞게 자르고, 모자란 부분을 패딩 0으로 채움\r\n","    input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\r\n","\r\n","    # 어텐션 마스크 초기화\r\n","    attention_masks = []\r\n","\r\n","    # 어텐션 마스크를 패딩이 아니면 1, 패딩이면 0으로 설정\r\n","    # 패딩 부분은 BERT 모델에서 어텐션을 수행하지 않아 속도 향상\r\n","    for seq in input_ids:\r\n","        seq_mask = [float(i>0) for i in seq]\r\n","        attention_masks.append(seq_mask)\r\n","\r\n","    # 데이터를 파이토치의 텐서로 변환\r\n","    inputs = torch.tensor(input_ids)\r\n","    masks = torch.tensor(attention_masks)\r\n","\r\n","    return inputs, masks"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_WgkAtMuFnZU"},"source":["# 문장 테스트\r\n","\r\n","def test_sentences(sentences):\r\n","\r\n","    # 평가모드로 변경\r\n","    model.eval()\r\n","\r\n","    # 문장을 입력 데이터로 변환\r\n","    inputs, masks = convert_input_data(sentences)\r\n","\r\n","    # 데이터를 GPU에 넣음\r\n","    b_input_ids = inputs.to(device)\r\n","    b_input_mask = masks.to(device)\r\n","            \r\n","    # 그래디언트 계산 안함\r\n","    with torch.no_grad():     \r\n","        # Forward 수행\r\n","        outputs = model(b_input_ids, \r\n","                        token_type_ids=None, \r\n","                        attention_mask=b_input_mask)\r\n","\r\n","    # 로스 구함\r\n","    logits = outputs[0]\r\n","\r\n","    # CPU로 데이터 이동\r\n","    logits = logits.detach().cpu().numpy()\r\n","\r\n","    return logits"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":241},"id":"c8jCZRwmFwg6","executionInfo":{"status":"error","timestamp":1614668148452,"user_tz":-540,"elapsed":1801,"user":{"displayName":"‍박준민(학부학생/상경대학 응용통계학과)","photoUrl":"","userId":"13006511384417057537"}},"outputId":"202692f9-68ce-432e-dbb8-572d1db11bbd"},"source":["logits = test_sentences([\"연기는 별로지만 재미 하나는 끝내줌!\"])\r\n","\r\n","print(logits)\r\n","print(np.argmax(logits))"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-8d3c2bb5868a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"I bought it to read comics and it's perfect for that. Multitasking with Chrome (1-3 tabs), comixology, hoopla, gmail, and kindle unlimited is smooth. It can't play HD video. Not a problem for me but if you plan to stream you'll be disappointed.\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'test_sentences' is not defined"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5qI5q9eaFyxo","executionInfo":{"status":"ok","timestamp":1614610300865,"user_tz":-540,"elapsed":678,"user":{"displayName":"‍박준민(학부학생/상경대학 응용통계학과)","photoUrl":"","userId":"13006511384417057537"}},"outputId":"04423e8f-e7af-4dde-de90-10436fb3889f"},"source":["logits = test_sentences(['주연배우가 아깝다. 총체적 난국...'])\r\n","\r\n","print(logits)\r\n","print(np.argmax(logits))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[ 3.413857 -3.051219]]\n","0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"NFkJNRfkRfx9"},"source":["Reference - https://github.com/deepseasw/bert-naver-movie-review"]}]}